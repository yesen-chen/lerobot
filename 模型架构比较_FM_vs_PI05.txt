================================================================================
                    模型架构比较: ILStudio FM vs LeRobot PI05
================================================================================

================================================================================
一、ILStudio Flow Matching (FM) 模型架构
================================================================================
文件: ILStudio/policy/fm/modeling.py
基于: Conditional Flow Matching (Lipman et al., 2023)
特点: 轻量级、纯状态输入、MLP backbone

┌─────────────────────────────────────────────────────────────────────────────┐
│                         FlowMatchingPolicy                                   │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  输入: state [B, state_dim]     (无图像、无语言)                              │
│                                                                              │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │ 训练阶段 (forward with action)                                        │   │
│  │                                                                       │   │
│  │   t ~ U[ε, 1]          noise ~ N(0, I)                                │   │
│  │   a_t = t·action + (1-t)·noise                                        │   │
│  │   u_t = action - noise  (目标速度)                                    │   │
│  │                                                                       │   │
│  │   ┌──────────────┐                                                    │   │
│  │   │ VelocityModel │ ────────────────────────────────────────────────► │   │
│  │   │   v_pred      │        Loss = MSE(v_pred, u_t)                    │   │
│  │   └──────────────┘                                                    │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
│                                                                              │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │ 推理阶段 (forward without action)                                     │   │
│  │                                                                       │   │
│  │   a_0 ~ N(0, I)                                                       │   │
│  │   ODE/Euler: da/dt = v_θ(a_t, t, s)                                   │   │
│  │   t: 0 → 1 积分  →  a_1 = 预测动作                                    │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
│                                                                              │
└─────────────────────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────────────────────┐
│                       VelocityModel (核心网络)                                │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  输入拼接: [a_t | time_emb | state]                                          │
│            (action_dim + time_dim + state_dim)                               │
│                                                                              │
│  ┌─────────────────┐                                                        │
│  │ SinusoidalTime   │   t [B]  ──►  time_emb [B, time_dim]                   │
│  │ Embedding        │           (正弦位置编码)                                │
│  └─────────────────┘                                                        │
│           │                                                                  │
│           ▼                                                                  │
│  ┌─────────────────────────────────────────────────────────────────────┐    │
│  │ concat(a_t, time_emb, state)  ──►  MLP  ──►  v [B, action_dim]       │    │
│  │                                                                      │    │
│  │  MLP 结构:                                                           │    │
│  │  Linear(input_dim, hidden_dim) → SiLU → ... (× num_layers)           │    │
│  │  → Linear(hidden_dim, action_dim)                                    │    │
│  └─────────────────────────────────────────────────────────────────────┘    │
│                                                                              │
└─────────────────────────────────────────────────────────────────────────────┘


================================================================================
================================================================================
二、LeRobot PI05 模型架构 (详细版)
================================================================================
================================================================================

文件: src/lerobot/policies/pi05/modeling_pi05.py
      src/lerobot/policies/pi05/configuration_pi05.py
基于: OpenPI (Physical Intelligence)
论文: π0.5 — PaliGemma VLM + Gemma Action Expert + Flow Matching
特点: 多模态(视觉+语言)、双分支Transformer、ADA-RMS时间条件、KV-Cache推理


================================================================================
2.1  总体架构鸟瞰图
================================================================================

┌─────────────────────────────────────────────────────────────────────────────────┐
│                           PI05Policy (最外层)                                    │
│  config_class = PI05Config    name = "pi05"                                     │
├─────────────────────────────────────────────────────────────────────────────────┤
│                                                                                  │
│  ┌─────────────────────────────────────────────────────────────────────────┐    │
│  │                   PI05Pytorch (核心模型)                                   │    │
│  │                                                                          │    │
│  │  ┌───────────────────────────────────────────────────────────────────┐  │    │
│  │  │         PaliGemmaWithExpertModel (双分支 Transformer)               │  │    │
│  │  │                                                                    │  │    │
│  │  │  ┌─────────────────────┐    ┌─────────────────────────────┐      │  │    │
│  │  │  │  PaliGemma VLM      │    │  Gemma Action Expert         │      │  │    │
│  │  │  │  (gemma_2b, 2.7B)   │    │  (gemma_300m, 300M)          │      │  │    │
│  │  │  │                     │    │                               │      │  │    │
│  │  │  │  - SigLIP Vision   │    │  - 无 embed_tokens            │      │  │    │
│  │  │  │  - Gemma Language   │    │  - 18层 Gemma Transformer     │      │  │    │
│  │  │  │  - 18层 Transformer │    │  - ADA-RMS (use_adarms=True) │      │  │    │
│  │  │  │  - use_adarms=False │    │  - GQA: 8 heads, 1 kv_head  │      │  │    │
│  │  │  └─────────────────────┘    └─────────────────────────────┘      │  │    │
│  │  └───────────────────────────────────────────────────────────────────┘  │    │
│  │                                                                          │    │
│  │  action_in_proj:  Linear(max_action_dim=32, expert_width=1024)           │    │
│  │  action_out_proj: Linear(expert_width=1024, max_action_dim=32)           │    │
│  │  time_mlp_in:     Linear(expert_width=1024, expert_width=1024)           │    │
│  │  time_mlp_out:    Linear(expert_width=1024, expert_width=1024)           │    │
│  └─────────────────────────────────────────────────────────────────────────┘    │
│                                                                                  │
│  rtc_processor: (可选) Real-Time Chunking 处理器                                 │
│  _action_queue: deque(maxlen=n_action_steps=50)                                 │
│                                                                                  │
└─────────────────────────────────────────────────────────────────────────────────┘


================================================================================
2.2  Gemma 模型变体配置
================================================================================

  ┌──────────────┬──────────────────────────────┬──────────────────────────────┐
  │   参数       │  gemma_2b (VLM PaliGemma)     │  gemma_300m (Action Expert)  │
  ├──────────────┼──────────────────────────────┼──────────────────────────────┤
  │ width        │  2048                         │  1024                        │
  │ depth        │  18 层                        │  18 层                       │
  │ mlp_dim      │  16,384                       │  4,096                       │
  │ num_heads    │  8                            │  8                           │
  │ num_kv_heads │  1 (GQA)                      │  1 (GQA)                     │
  │ head_dim     │  256                          │  256                         │
  │ use_adarms   │  False                        │  True (时间条件注入)         │
  │ vocab_size   │  257,152                      │  257,152                     │
  │ activation   │  gelu_pytorch_tanh            │  gelu_pytorch_tanh           │
  └──────────────┴──────────────────────────────┴──────────────────────────────┘

  PaliGemma VLM 额外组件:
  - SigLIP Vision Tower: image_size=224, intermediate=4304, proj_dim=2048
  - embed_tokens: 词嵌入层 (vocab=257,152)
  
  Action Expert 特殊设置:
  - embed_tokens = None (不需要词嵌入，输入由 action_in_proj 提供)
  - use_adarms = True (时间步通过 ADA-RMS 条件化注入每层 LayerNorm)


================================================================================
2.3  输入预处理详细流程
================================================================================

  ┌─────────────────────────────────────────────────────────────────────────────┐
  │  原始输入                                                                    │
  │  ├─ images:  [B, C, H, W] (可多相机) 值域 [0, 1]                            │
  │  ├─ tokens:  [B, seq_len]  语言指令 token ids                                │
  │  ├─ masks:   [B, seq_len]  语言注意力掩码                                    │
  │  └─ actions: [B, chunk_size=50, action_dim] (仅训练时)                        │
  └─────────────────────────────────────────────────────────────────────────────┘
           │
           ▼
  ┌─────────────────────────────────────────────────────────────────────────────┐
  │  _preprocess_images                                                          │
  │  1. [B,C,H,W] → permute → [B,H,W,C]                                        │
  │  2. resize_with_pad_torch → 224×224 (保持比例，黑边填充)                      │
  │  3. 归一化: [0,1] → [-1,1]  (SigLIP 要求)                                   │
  │  4. permute 回 [B,C,H,W]                                                    │
  │  5. 缺失相机: 用 -1 填充的全黑图 + mask=0                                    │
  └─────────────────────────────────────────────────────────────────────────────┘
           │
           ▼
  ┌─────────────────────────────────────────────────────────────────────────────┐
  │  prepare_action (仅训练)                                                      │
  │  pad_vector(actions, max_action_dim=32)                                       │
  │  若 action_dim < 32，末尾补零到 32 维                                         │
  └─────────────────────────────────────────────────────────────────────────────┘


================================================================================
2.4  embed_prefix — Prefix 嵌入 (图像+语言)
================================================================================

  ┌─────────────────────────────────────────────────────────────────────────────┐
  │  embed_prefix(images, img_masks, tokens, masks)                              │
  │                                                                              │
  │  图像嵌入 (每个相机独立处理):                                                  │
  │  ┌────────────────────────────────────────────────────────────────────────┐ │
  │  │  image [B,C,224,224]                                                    │ │
  │  │    │                                                                    │ │
  │  │    ▼                                                                    │ │
  │  │  SigLIP Vision Tower                                                    │ │
  │  │    │ patch_embedding → position_embedding → transformer_encoder         │ │
  │  │    ▼                                                                    │ │
  │  │  img_emb [B, num_patches, 2048]                                         │ │
  │  │  (224/14)² = 256 个 patch → num_patches = 256                           │ │
  │  │                                                                         │ │
  │  │  pad_mask: img_mask[:, None].expand(B, 256)                             │ │
  │  │  att_mask: [0] × 256  (双向注意力)                                       │ │
  │  └────────────────────────────────────────────────────────────────────────┘ │
  │                                                                              │
  │  语言嵌入:                                                                    │
  │  ┌────────────────────────────────────────────────────────────────────────┐ │
  │  │  tokens [B, seq_len]                                                    │ │
  │  │    │                                                                    │ │
  │  │    ▼                                                                    │ │
  │  │  paligemma.language_model.embed_tokens(tokens)                          │ │
  │  │    │                                                                    │ │
  │  │    ▼                                                                    │ │
  │  │  lang_emb × sqrt(embed_dim)   [B, seq_len, 2048]                        │ │
  │  │  (缩放因子与 Gemma 的 embedding 约定一致)                                │ │
  │  │                                                                         │ │
  │  │  pad_mask: masks (来自 tokenizer)                                        │ │
  │  │  att_mask: [0] × seq_len  (双向注意力)                                   │ │
  │  └────────────────────────────────────────────────────────────────────────┘ │
  │                                                                              │
  │  拼接:                                                                        │
  │  prefix_embs = cat([img_emb_cam1, img_emb_cam2, ..., lang_emb], dim=1)       │
  │  prefix_pad_masks = cat([img_mask_1, img_mask_2, ..., lang_mask], dim=1)     │
  │  prefix_att_masks = [0,0,...,0]  ← 全部为 0 表示 prefix 内部双向可见          │
  │                                                                              │
  │  最终 prefix 序列长度 = N_cam × 256 + seq_len                                │
  │  例: 2 相机 = 2×256 + 200 = 712 tokens                                      │
  └─────────────────────────────────────────────────────────────────────────────┘


================================================================================
2.5  embed_suffix — Suffix 嵌入 (噪声动作+时间)
================================================================================

  ┌─────────────────────────────────────────────────────────────────────────────┐
  │  embed_suffix(noisy_actions, timestep)                                       │
  │                                                                              │
  │  ┌───────────────────────────────────────────────────────────────────────┐  │
  │  │  时间嵌入:                                                              │  │
  │  │  timestep [B]                                                           │  │
  │  │    │                                                                    │  │
  │  │    ▼                                                                    │  │
  │  │  create_sinusoidal_pos_embedding(t, dim=1024,                           │  │
  │  │         min_period=4e-3, max_period=4.0)                                │  │
  │  │    │                                                                    │  │
  │  │    ▼                                                                    │  │
  │  │  time_emb [B, 1024]  (sin/cos 频率编码)                                 │  │
  │  │    │                                                                    │  │
  │  │    ▼                                                                    │  │
  │  │  time_mlp_in:  Linear(1024, 1024) ──► SiLU                              │  │
  │  │  time_mlp_out: Linear(1024, 1024) ──► SiLU                              │  │
  │  │    │                                                                    │  │
  │  │    ▼                                                                    │  │
  │  │  adarms_cond [B, 1024]  (用于 Expert 每层 ADA-RMS 条件化)               │  │
  │  └───────────────────────────────────────────────────────────────────────┘  │
  │                                                                              │
  │  ┌───────────────────────────────────────────────────────────────────────┐  │
  │  │  动作嵌入:                                                              │  │
  │  │  noisy_actions [B, chunk_size=50, max_action_dim=32]                    │  │
  │  │    │                                                                    │  │
  │  │    ▼                                                                    │  │
  │  │  action_in_proj: Linear(32, 1024)                                       │  │
  │  │    │                                                                    │  │
  │  │    ▼                                                                    │  │
  │  │  action_emb [B, 50, 1024]                                               │  │
  │  └───────────────────────────────────────────────────────────────────────┘  │
  │                                                                              │
  │  注意: time_emb 不直接拼入 action_emb，而是作为 adarms_cond                   │
  │        传递给 Expert Transformer 的每一层 LayerNorm                           │
  │                                                                              │
  │  suffix_embs = action_emb  [B, 50, 1024]                                    │
  │  suffix_pad_masks = ones [B, 50]                                             │
  │  suffix_att_masks = [1, 0, 0, ..., 0]  (50 tokens)                          │
  │     ↑ 第一个为 1: prefix 不 attend 到 action tokens                          │
  │       后续为 0: action tokens 之间双向可见                                    │
  │                                                                              │
  │  返回: suffix_embs, suffix_pad_masks, suffix_att_masks, adarms_cond          │
  └─────────────────────────────────────────────────────────────────────────────┘


================================================================================
2.6  注意力掩码机制 (Prefix-LM 风格)
================================================================================

  make_att_2d_masks(pad_masks, att_masks) → 2D 注意力矩阵

  att_masks = [prefix: 0,0,...,0 | suffix: 1,0,0,...,0]
  cumsum =    [prefix: 0,0,...,0 | suffix: 1,1,1,...,1]

  规则: cumsum[query] <= cumsum[key] 时可以 attend

  最终效果:
  ┌────────────────────────────────┬────────────────────────┐
  │        Key: Prefix             │     Key: Suffix        │
  ┌────────────────────────────────┬────────────────────────┤
  │ Query:     │ ✓ 全部双向可见    │  ✗ 看不到 suffix       │
  │ Prefix     │ (图像⟷语言)      │                        │
  ├────────────┼────────────────────┼────────────────────────┤
  │ Query:     │ ✓ 可以 attend     │  ✓ suffix 内部         │
  │ Suffix     │   到全部 prefix   │    双向可见            │
  └────────────┴────────────────────┴────────────────────────┘

  即: Prefix-LM 注意力。prefix 是纯双向的上下文；suffix 可以看到
      prefix + 自身，但 prefix 看不到 suffix。


================================================================================
2.7  PaliGemmaWithExpertModel — 双分支 Transformer 详细
================================================================================

  PaliGemmaWithExpertModel.forward 有三种模式:

  模式 A: inputs_embeds = [prefix_embs, None]   → 仅跑 PaliGemma (推理缓存 prefix)
  模式 B: inputs_embeds = [None, suffix_embs]   → 仅跑 Expert   (推理去噪步)
  模式 C: inputs_embeds = [prefix_embs, suffix_embs] → 联合 (训练)

  ═══════ 模式 C (训练时联合前向): compute_layer_complete ═══════

  对每一层 layer_idx (共 18 层):
  ┌─────────────────────────────────────────────────────────────────────────────┐
  │  Layer[layer_idx]                                                            │
  │                                                                              │
  │  ┌── 分支 0: PaliGemma ──┐  ┌── 分支 1: Gemma Expert ──┐                   │
  │  │                        │  │                            │                   │
  │  │  input_layernorm       │  │  input_layernorm           │                   │
  │  │  (cond=None, 标准RMS)  │  │  (cond=adarms_cond,       │                   │
  │  │  → hidden, gate_0      │  │   ADA-RMS 条件化)          │                   │
  │  │                        │  │  → hidden, gate_1           │                   │
  │  │  Q_0 = q_proj(hidden)  │  │  Q_1 = q_proj(hidden)      │                   │
  │  │  K_0 = k_proj(hidden)  │  │  K_1 = k_proj(hidden)      │                   │
  │  │  V_0 = v_proj(hidden)  │  │  V_1 = v_proj(hidden)      │                   │
  │  └────────┬───────────────┘  └────────┬───────────────────┘                   │
  │           │                            │                                      │
  │           └──── 拼接 Q, K, V ─────────┘                                      │
  │                    │                                                          │
  │                    ▼                                                          │
  │  ┌──────────────────────────────────────────────────────────────────────┐   │
  │  │  Q_cat = cat([Q_0, Q_1], dim=seq)    [B, heads, prefix+suffix, d]    │   │
  │  │  K_cat = cat([K_0, K_1], dim=seq)                                    │   │
  │  │  V_cat = cat([V_0, V_1], dim=seq)                                    │   │
  │  │                                                                      │   │
  │  │  apply RoPE (Rotary Position Embedding)                              │   │
  │  │  Q_cat, K_cat = apply_rotary_pos_emb(Q_cat, K_cat, cos, sin)        │   │
  │  │                                                                      │   │
  │  │  ┌────────────────────────────────────────────────────────────────┐ │   │
  │  │  │  联合 Attention                                                  │ │   │
  │  │  │  att_output = softmax(Q·K^T / √d + mask) · V                    │ │   │
  │  │  │  att_output [B, 8 heads, prefix+suffix, 256]                     │ │   │
  │  │  │  → reshape [B, prefix+suffix, 8×256=2048]                        │ │   │
  │  │  └────────────────────────────────────────────────────────────────┘ │   │
  │  └──────────────────────────────────────────────────────────────────────┘   │
  │                    │                                                          │
  │           ┌────────┴────────┐                                                │
  │           ▼                 ▼                                                 │
  │  ┌── 分支 0 ──────┐  ┌── 分支 1 ──────────┐                                 │
  │  │ o_proj(att_out  │  │ o_proj(att_out      │                                 │
  │  │   [:, :prefix]) │  │   [:, prefix:])     │                                 │
  │  │ + gated_residual│  │ + gated_residual    │                                 │
  │  │   (gate_0)      │  │   (gate_1)          │                                 │
  │  │                 │  │                      │                                 │
  │  │ post_attn_norm  │  │ post_attn_norm      │                                 │
  │  │ (cond=None)     │  │ (cond=adarms_cond)  │                                 │
  │  │                 │  │                      │                                 │
  │  │ MLP (GeGLU)     │  │ MLP (GeGLU)         │                                 │
  │  │ up:2048→16384   │  │ up:1024→4096        │                                 │
  │  │ gate:2048→16384 │  │ gate:1024→4096      │                                 │
  │  │ down:16384→2048 │  │ down:4096→1024      │                                 │
  │  │                 │  │                      │                                 │
  │  │ + gated_residual│  │ + gated_residual     │                                 │
  │  └────────┬────────┘  └────────┬─────────────┘                                │
  │           │                    │                                               │
  │           ▼                    ▼                                               │
  │  outputs_embeds = [prefix_out, suffix_out]                                    │
  │  → 传递到下一层                                                                │
  └─────────────────────────────────────────────────────────────────────────────┘

  经过 18 层后:
  final_norm (各自分支独立):
    prefix_out = VLM.norm(prefix_hidden)
    suffix_out = Expert.norm(suffix_hidden, cond=adarms_cond)


================================================================================
2.8  ADA-RMS (Adaptive RMS Norm) — 时间条件注入机制
================================================================================

  标准 RMSNorm:
    y = x / RMS(x) * weight

  ADA-RMS (在 Expert 中使用):
    adarms_cond = time_mlp(sinusoidal_emb(t))  [B, 1024]
    gate = sigmoid(linear(adarms_cond))         [B, 1024]  
    y_norm = x / RMS(x) * weight                标准 RMS 输出
    y = gate * y_norm                            门控调制
    返回 (y, gate)                               gate 用于后续 gated_residual

  gated_residual(原始, 新增, gate):
    return 原始 + gate * 新增

  效果: 时间步 t 通过 ADA-RMS 控制 Expert 每一层的信息流大小


================================================================================
2.9  训练前向传播 (PI05Pytorch.forward)
================================================================================

  ┌─────────────────────────────────────────────────────────────────────────────┐
  │  PI05Pytorch.forward(images, img_masks, tokens, masks, actions)              │
  │                                                                              │
  │  Step 1: 采样噪声和时间                                                      │
  │    noise ~ N(0, 1)  [B, 50, 32]                                              │
  │    time ~ Beta(α=1.5, β=1.0) × 0.999 + 0.001   [B]                          │
  │                                                                              │
  │  Step 2: Flow Matching 插值                                                  │
  │    x_t = time · noise + (1 - time) · actions    [B, 50, 32]                  │
  │    u_t = noise - actions                         [B, 50, 32]  (目标速度)     │
  │                                                                              │
  │  Step 3: 嵌入                                                                │
  │    prefix_embs, ... = embed_prefix(images, tokens)                           │
  │    suffix_embs, ..., adarms_cond = embed_suffix(x_t, time)                   │
  │                                                                              │
  │  Step 4: 构造注意力掩码                                                      │
  │    pad_masks = cat([prefix_pad, suffix_pad])                                 │
  │    att_masks = cat([prefix_att, suffix_att])                                 │
  │    att_2d = make_att_2d_masks(pad_masks, att_masks)  → 4D mask               │
  │    position_ids = cumsum(pad_masks) - 1                                      │
  │                                                                              │
  │  Step 5: 联合前向 (模式 C)                                                   │
  │    (_, suffix_out), _ = PaliGemmaWithExpert(                                 │
  │        inputs_embeds=[prefix_embs, suffix_embs],                             │
  │        attention_mask=att_2d_4d,                                             │
  │        adarms_cond=[None, adarms_cond]  ← VLM 不用 adarms                   │
  │    )                                                                         │
  │                                                                              │
  │  Step 6: 输出投影                                                            │
  │    suffix_out = suffix_out[:, -50:]  取最后 50 步                             │
  │    v_t = action_out_proj(suffix_out)  [B, 50, 32]                            │
  │                                                                              │
  │  Step 7: 损失                                                                │
  │    loss = MSE(u_t, v_t, reduction="none")  [B, 50, 32]                       │
  │    最终 loss = mean(loss[:, :, :actual_action_dim])                           │
  └─────────────────────────────────────────────────────────────────────────────┘


================================================================================
2.10  推理采样 (PI05Pytorch.sample_actions) — KV-Cache 加速
================================================================================

  ┌─────────────────────────────────────────────────────────────────────────────┐
  │  sample_actions(images, img_masks, tokens, masks, num_steps=10)              │
  │                                                                              │
  │  ═══ 阶段 1: 预计算 Prefix (只跑一次) ═══                                   │
  │                                                                              │
  │  prefix_embs = embed_prefix(images, tokens)                                 │
  │                                                                              │
  │  PaliGemmaWithExpert.forward(                                                │
  │      inputs_embeds=[prefix_embs, None],   ← 模式 A                          │
  │      use_cache=True                                                          │
  │  )                                                                           │
  │  → past_key_values (18层的 KV cache, 存储 prefix 的 K,V)                     │
  │                                                                              │
  │  ═══ 阶段 2: 迭代去噪 (跑 num_steps=10 次) ═══                              │
  │                                                                              │
  │  x_t = noise ~ N(0,1)  [B, 50, 32]                                          │
  │  dt = -1/10 = -0.1                                                           │
  │                                                                              │
  │  for step in [0, 1, ..., 9]:                                                 │
  │      t = 1.0 + step × (-0.1)   →   t ∈ {1.0, 0.9, 0.8, ..., 0.1}          │
  │      │                                                                       │
  │      ▼                                                                       │
  │  ┌─────────────────────────────────────────────────────────────────────┐    │
  │  │  denoise_step(prefix_pad_masks, past_key_values, x_t, timestep=t)    │    │
  │  │                                                                       │    │
  │  │  1. embed_suffix(x_t, t) → suffix_embs, adarms_cond                  │    │
  │  │  2. 构造注意力掩码:                                                    │    │
  │  │     prefix 部分用 past_key_values (KV cache)                          │    │
  │  │     suffix 部分拼接 prefix_pad_2d + suffix_att_2d                     │    │
  │  │  3. position_ids = prefix_offset + cumsum(suffix_pad)                 │    │
  │  │  4. PaliGemmaWithExpert.forward(                                      │    │
  │  │         inputs_embeds=[None, suffix_embs],  ← 模式 B                  │    │
  │  │         past_key_values=past_key_values,    ← 复用 prefix KV          │    │
  │  │         adarms_cond=[None, adarms_cond]                               │    │
  │  │     )                                                                 │    │
  │  │  5. suffix_out[:, -50:]                                               │    │
  │  │  6. v_t = action_out_proj(suffix_out)  [B, 50, 32]                    │    │
  │  └─────────────────────────────────────────────────────────────────────┘    │
  │      │                                                                       │
  │      ▼                                                                       │
  │      x_t = x_t + dt × v_t    (Euler 步进, dt=-0.1)                          │
  │      即 x_t = x_t - 0.1 × v_t                                               │
  │                                                                              │
  │  最终: x_0 ≈ actions  [B, 50, 32]                                            │
  │  裁剪: actions[:, :, :actual_action_dim]                                     │
  └─────────────────────────────────────────────────────────────────────────────┘

  KV-Cache 的效果:
  - Prefix (图像+语言) 只编码 1 次，生成 KV 缓存
  - 10 次去噪步中，每步只需要跑 Expert 的 50 个 action token
  - 大幅减少推理计算量 (prefix 可能有 700+ tokens)


================================================================================
三、核心差异对比表
================================================================================

┌──────────────────┬──────────────────────────────┬──────────────────────────────┐
│     特性         │  ILStudio FM                   │  LeRobot PI05                │
├──────────────────┼──────────────────────────────┼──────────────────────────────┤
│ 输入模态         │ 纯状态 (state only)            │ 图像 + 语言 + state          │
├──────────────────┼──────────────────────────────┼──────────────────────────────┤
│ 核心 Backbone    │ MLP (3层 Linear+SiLU)          │ PaliGemma 2B + Gemma 300M   │
│                  │                                │ (双分支 Transformer)         │
├──────────────────┼──────────────────────────────┼──────────────────────────────┤
│ 视觉编码器       │ 无                             │ SigLIP (224×224, 256 patches)│
├──────────────────┼──────────────────────────────┼──────────────────────────────┤
│ 时间编码         │ SinusoidalTimeEmbedding        │ sin/cos → time_mlp → ADA-RMS│
│ 注入方式         │ 拼接进 MLP 输入                │ 条件化 Expert 每层 LayerNorm │
├──────────────────┼──────────────────────────────┼──────────────────────────────┤
│ 时间采样(训练)   │ t ~ Uniform(ε, 1)              │ t ~ Beta(1.5, 1.0)×0.999    │
├──────────────────┼──────────────────────────────┼──────────────────────────────┤
│ 插值方向         │ a_t = t·act + (1-t)·noise      │ x_t = t·noise + (1-t)·act   │
│                  │ t=0:噪声  t=1:动作             │ t=0:动作  t=1:噪声           │
├──────────────────┼──────────────────────────────┼──────────────────────────────┤
│ 推理方向         │ t: 0→1 (正向积分)              │ t: 1→0 (反向积分)            │
├──────────────────┼──────────────────────────────┼──────────────────────────────┤
│ 推理步数         │ 可配 (默认100, ODE自适应)      │ 10 步 Euler                  │
├──────────────────┼──────────────────────────────┼──────────────────────────────┤
│ 推理加速         │ ODE solver (dopri5)            │ KV-Cache (prefix 只算 1 次)  │
├──────────────────┼──────────────────────────────┼──────────────────────────────┤
│ 动作 chunk       │ chunk_size (默认1)             │ chunk_size = 50              │
├──────────────────┼──────────────────────────────┼──────────────────────────────┤
│ 注意力类型       │ 无 (MLP 无注意力)              │ Prefix-LM (prefix双向,       │
│                  │                                │ suffix可见prefix+自身)       │
├──────────────────┼──────────────────────────────┼──────────────────────────────┤
│ 模型规模         │ ~百KB (hidden_dim=256)          │ ~3B 参数                     │
├──────────────────┼──────────────────────────────┼──────────────────────────────┤
│ 预训练           │ 无 (从头训练)                   │ PaliGemma 预训练 VLM         │
├──────────────────┼──────────────────────────────┼──────────────────────────────┤
│ 微调策略         │ 全参数训练                     │ 可冻结 vision / 仅训练 expert│
├──────────────────┼──────────────────────────────┼──────────────────────────────┤
│ 适用场景         │ 低维状态、快速训练/推理         │ 视觉语言机器人、复杂泛化任务 │
└──────────────────┴──────────────────────────────┴──────────────────────────────┘


================================================================================
四、PI05 端到端数据流总图
================================================================================

【训练】
  images ──► SigLIP ──► img_embs ──┐
  tokens ──► Embed×√d ──► lang_embs─┤
                                    ├──► prefix_embs [B, ~712, 2048]
                                    │
  actions ──► pad(32) ──────────────┤
  noise ~ N(0,1) ──────────────────┤
  time ~ Beta(1.5,1) ─────────────┤
    │                               │
    ├─► x_t = t·noise + (1-t)·act  │
    ├─► u_t = noise - act           │
    │                               │
    ├─► action_in_proj(x_t) ──► suffix_embs [B, 50, 1024]
    └─► time_mlp(sin/cos(t)) ──► adarms_cond [B, 1024]
                                    │
      ┌─────────────────────────────┘
      │
      ▼
  PaliGemmaWithExpert (18 层联合 Attention)
  ├─ VLM 分支: prefix_embs → ... → prefix_out (不使用)
  └─ Expert 分支: suffix_embs + adarms_cond → ... → suffix_out
                                    │
                                    ▼
  suffix_out[:, -50:] ──► action_out_proj ──► v_t [B, 50, 32]
                                    │
                                    ▼
  Loss = MSE(u_t, v_t)[:, :, :actual_dim].mean()

【推理】
  images + tokens ──► embed_prefix ──► PaliGemma(use_cache=True)
                                       → past_key_values (KV 缓存)
  
  x_t = noise [B, 50, 32]
  for t in {1.0, 0.9, ..., 0.1}:
      suffix_embs = action_in_proj(x_t)
      adarms_cond = time_mlp(sin/cos(t))
      v_t = Expert(suffix_embs, past_kv, adarms_cond) → action_out_proj
      x_t = x_t + (-0.1) × v_t
  
  actions = x_t[:, :, :actual_dim]


================================================================================
五、总结
================================================================================

FM (ILStudio):
  - 极简 MLP 架构，纯状态输入
  - 适合低维、快速迭代的策略学习
  - Flow Matching: 正向积分 t:0→1, 学习速度场 v=act-noise

PI05 (LeRobot):
  - 大规模多模态架构: SigLIP + PaliGemma VLM (2B) + Gemma Expert (300M)
  - 双分支 Transformer: VLM 处理视觉语言，Expert 处理动作
  - 共享注意力: prefix(视觉+语言) ↔ suffix(动作) 信息交互
  - ADA-RMS: 时间步通过门控 LayerNorm 条件化注入 Expert 每一层
  - KV-Cache 推理: prefix 只编码 1 次，10 步去噪仅跑 Expert
  - Flow Matching: 反向积分 t:1→0, 学习速度场 v=noise-act
  - 适合视觉语言驱动的复杂机器人操控任务

两者核心都是 Flow Matching, 但 PI05 将其嵌入到了 VLM 架构中,
实现了从视觉语言理解到精细动作生成的端到端流程。
